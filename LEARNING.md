# Learning Path for Data Scientists

Presenting a comprehensive list of resources for learning about LLMs. If you find something that should be added, please submit an Issue/PR.

Topic wise resources are listed below:
- [Organized Courses](#organized-courses)
- [LLM Publications](#llm-publications)
- [Community Content - LLMs](#community-content---llms)
- [Community Content - Prompt Engineering](#community-content---prompt-engineering)
- [GitHub Repositories](#github-repositories)

## Organized Courses
Some courses gaining popularity in the community are listed below:
- [Transformers United, Stanford University](https://web.stanford.edu/class/cs25/) [youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM) 
- [Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)
- [ChatGPT Prompt Engineering for Developers, deeplearning.ai](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)

## LLM Publications

Papers based on transformer architecture:
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805.pdf)
- [GPT: Improving Language Understanding by Generative Pre-Training](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/abs/2205.01068)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165.pdf)
- [GPT-Neo: The Open Source GPT-3](https://arxiv.org/abs/2105.14165.pdf)
- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683.pdf)
- [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/abs/2204.02311)
- [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/abs/2211.05100)
- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990)
- [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](http://arxiv.org/abs/2112.11446v2)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)
- [LaMDA: Language Models for Dialog Applications](https://arxiv.org/abs/2201.08239)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971v1)
- [GPT-4 Technical Report](http://arxiv.org/abs/2303.08774v2)
- [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/abs/2303.17564)
- [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)


## Community Content - LLMs

Listing some of the best community content for learning about LLMs.

YouTube Tutorials:
- [The Transformer Explained](https://www.youtube.com/watch?v=4Bdc55j80l8)
- [The Illustrated Transformer](https://www.youtube.com/watch?v=4Bdc55j80l8)
- [The Illustrated BERT, ELMo, and co.](https://www.youtube.com/watch?v=FKlPCK1uFrc)
- [The Illustrated GPT-3](https://www.youtube.com/watch?v=SY5PvZrJhLE)
- [The Illustrated GPT-Neo](https://www.youtube.com/watch?v=SY5PvZrJhLE)

Blogs:
- [Large Language Models: A New Moore's Law?](https://huggingface.co/blog/large-language-models)
- [Why did all of the public reproduction of GPT-3 fail? In which tasks should we use GPT-3.5/ChatGPT?](https://jingfengyang.github.io/gpt)
- [The Promise and Perils of Large Language Models](https://twosigmaventures.com/blog/article/the-promise-and-perils-of-large-language-models/?ref=assemblyai.com)
- [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://arxiv.org/pdf/2304.13712.pdf)
- [The illustrated transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)
- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)
- [The Illustrated GPT-3](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)

## Community Content - Prompt Engineering

List some of useful resources on prompt engineering:
- [Prompt Engineering Guide](https://www.promptingguide.ai)
- [ChatGPT-Data-Science-Prompts](https://github.com/travistangvh/ChatGPT-Data-Science-Prompts)
- [Guide to prompt engineering](https://www.youtube.com/watch?v=ydjRYmM19DY)

## GitHub Repositories

- [Langchain](https://github.com/hwchase17/langchain/)
- [CodeGenie](https://github.com/thismlguy/code-genie)
- [LLM Practical Guide](https://github.com/Mooler0410/LLMsPracticalGuide)
