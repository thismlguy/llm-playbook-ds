# Learning Path for Data Scientists

Presenting a comprehensive list of resources for learning about LLMs. If you find something that should be added, please submit an Issue/PR.

Topic wise resources are listed below:
- [Understanding LLMs](#understanding-llms)
- [Prompt Engineering](#prompt-engineering)

Some courses gaining popularity in the community are listed below:
- [Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)
- 

## Understanding LLMs

Here are some resources to help you understand how LLMs work:

University Courses:
- [Transformers United, Stanford University](https://web.stanford.edu/class/cs25/) [youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)

Papers based on transformer architecture:
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762.pdf)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805.pdf)
- [GPT-2: Language Models are Unsupervised Multitask Learners](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165.pdf)
- [GPT-Neo: The Open Source GPT-3](https://arxiv.org/abs/2105.14165.pdf)
- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683.pdf)
- [XLNet: Generalized Autoregressive Pretraining for Language Understanding](https://arxiv.org/abs/1906.08237.pdf)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942.pdf)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach](https://arxiv.org/abs/1907.11692.pdf)
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators](https://arxiv.org/abs/2003.10555.pdf)
- [Reformer: The Efficient Transformer](https://arxiv.org/abs/2001.04451.pdf)
- [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150.pdf)

YouTube Tutorials:
- [The Transformer Explained](https://www.youtube.com/watch?v=4Bdc55j80l8)
- [The Illustrated Transformer](https://www.youtube.com/watch?v=4Bdc55j80l8)
- [The Illustrated BERT, ELMo, and co.](https://www.youtube.com/watch?v=FKlPCK1uFrc)
- [The Illustrated GPT-3](https://www.youtube.com/watch?v=SY5PvZrJhLE)
- [The Illustrated GPT-Neo](https://www.youtube.com/watch?v=SY5PvZrJhLE)

Individual blogs:
- [The illustrated transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)
- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)
- [The Illustrated GPT-3](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)
- [The Illustrated GPT-Neo](https://jalammar.github.io/illustrated-gpt3/)


# Prompt Engineering

Tutorials:
- [ChatGPT Prompt Engineering for Developers, deeplearning.ai](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)

Papers:
- [PPLM: Plug and Play Language Models for Conditional Text Generation](https://arxiv.org/abs/1912.02164.pdf)
- 
